I want to build an "anime tiktok reproducer" project. This will allow me to remaster my tiktok for another short platform so it isn't too similar. It will follow a process I will describe as precisely as possible. If at any moment you have any uncertainty, ask me clarifying questions.

GOAL: We will take a tiktok presenting an anime and produce a premiere pro automated project that will prepare as much as possible of the rework of the original tiktok, by notably detecting which scenes were used and placing them correctly on premiere pro timeline.

ARCHITECTURE: I have already developed and validated a critical part of the project which is the submodule **anime_searcher**. It's a python program which indexes episodes of an anime and finds from a screenshot where it takes place (which episodes and at which timing) thanks to a SSCD + FAISS algorithm.
I now want to build the "bigger picture" by building an entire automated process, built around a **web app** we will run locally. Because project is ran in python you will build an app from the front framework of your choice + python will serve as backend (fastapi for example). The critical requirement of the app will be to be **dynamic / real time** by having a video timeline editor for scene placements. This means we will have a video player of our tiktok and we can navigate through our video (using play / pause / frame by frame controls). We will additionally have scenes (visually seen on the timeline thanks to a graphic element like backgrounds colors + scene number) which can be modified. Scene's modification dynamically changes timeline and video player's cursor is in sync with it all (highlights current scene, allows us to split a scene at current timing). This app allows me, the user, to be guided throughout the whole process and to interact with our pipeline whenever it is needed (additional input, manual validation needed...)

PROCESS:

- In input we need the tiktok URL + source episodes of the anime. If not provided our app will ask for them before starting.
- We will then download the tiktok using yt-dlp
- We will then use pyscenedetect library to automatically detect the scenes of the tiktok. It will naturally detect more scenes than what there really is. Sensibility is intended as we will manually validate scenes just after.
- After scene detection we display the "scene validation". It will be an interactive phase where the user will navigate across the tiktok video and validate scenes manually. He will be able to modify scenes (add, delete, merge). This serves so that a real human finalised scene detection and validates visually scenes. This allows us to be sure of the exactitude of scenes' slicing and their timings.
- After this phase we consider scenes are correctly cut with correct timings and we thus find their **timings** in the real episodes. For each scene we retrieve frames (beginning / middle / end) and use `anime_searcher` submodule to find the top 5 candidates for each frame. By doing this we will be able to **confirm** we have found the scene by looking at timestamps: there is necessarily a beginning candidate + middle candidate + end candidate that **follow themselves** (temporal continuity). Beware that tiktok often have sped up scenes (and even slowed down scenes) so don't strictly search for a 100% original speed precise temporal match but have a bit of permissiveness (range from 70% to 160% speed). Save each scene's found timings (beginning / end) for next phase
- Now that previous phase is done (display a loading animation meanwhile) you can show the "match validation" phase. It will serve to validate manually the found timings for clips. For each clip you will display side by side the original clip of the tiktok and the found one. It should be two players configured with the exact good timings. Under each clip in little you will display text corresponding to clip duration. For the found clip of real episodes also add to the next the approximate speed up estimation (compare both duration to deduce this).
- Now that all scenes are completely validated (scene segmenting + timing), we can go onto the video crafting phase. We run faster-whisper with timestamp word level transcription on our tiktok to know what the TTS voice says and when. Display a "start transcription" with a drop-down parameter configured to "auto". It will correspond to the language parameter. Besides 'auto' we can chose 'anglais', 'espagnol' and 'fran√ßais'. Start transcription when the users click on Start. You will internally format the transcription information to match **each scene**: this means our format (json for example) will be formatted by **scene** (each scene correspond to an entry with tts transcription + timings). When transcription is done, display an UI showing what the TTS voice says for each scene (each scene is a new line). The user can then **edit** (text field) the transcription if some words are miss transcribed. When done the user will confirm TTS transcription is valid.
- The app will then show a new page. Firstly show a copyable field or button (probably don't show everything at once as it will be long). It will contain an optimised **restructuration prompt** that will redo the script, targeting french as output language, so as to modify the original script so it won't be exactly the same so it won't feel like blunt copying. It needs to take into account each scene's TTS duration because we will regenerate a TTS with this new script and redo the montage scene by scene. If a scene's TTS becomes much longer or shorter compared to the original one it will lead to a big visual disparity so aim for approximate same audio duration for each scene's text (estimate spoken tts duration). This is because we will adapt each anime scene speed to match the TTS (so we want to stay in an acceptable range) in order to reproduce the tiktok but still be able to have a brand new TTS with a brand new script. Include our transcription json at the end of the prompt so the LLM has all information needed to process the prompt. Secondly it will have fields to **await** for two things: 1. text field which awaits a json containing the **new script** (same format as before but with reworked TTS sentences), we will only verify for json validity format in frontend 2. file field awaiting for an **audio file**, will correspond to the newly generated TTS of the reworked script
- All manual interactions are finished. We will only display loading animations from now on. The automated next phase is using `auto-editor` on our new TTS audio file. Here are example parameters I tested with good success on ElevenLabs audios: `--edit audio:threshold=0.05,stream=all --margin 0.04sec,0.04sec --silent-speed 99999`.
- We then do a faster-whisper transcription on this new file. You have access to its real script so you cannot fail on the transcription's content. This transcription is solely here to get **timings** of the words.
- Using the timings from faster-whisper transcription, we can go on the last phase: premiere pro project generation. You will generate a .jsx script which automate our project. First, you will create **markers** on the real timing where each text of a scene is said by the tts (one marker = one scene). You will then put each clip on its corresponding marker. Do not put every clip at once as they can overlap. You will process one clip at a time. For each clip you will try to make it last untill next marker (or end of TTS if it's the very last clip). You will either speed it up if it goes longer than next marker or slow it down if it's not long enough. You have no limits for speed up but you cannot slow it down more than 75%. Above this cap let a gap and process next clip. Generate a .srt subtitle file. Optimize it in length by targetting short format like tiktok. Import it automatically in our timeline. Bundle our .jsx script in a compressed archive alongside needed assets (TTS auto-editor'd audio file, **used only** source episodes, subtitle file). The composition should match short platform (vertical 1080x1920). I have Adobe Premiere Pro 2025.

SCENE VALIDATION: This section describes what features should be included in the scene validation phase. IMPLEMENTATION NOTE: For the timeline component, do not build the drag-and-drop and resizing logic from scratch using raw DOM manipulation. Use a specialized library (like 'vis-timeline', 'react-draggable', 'dnd-kit') or a robust framework capability that handles state synchronization cleanly.

- Video player of the tiktok
- A synchronised timeline under player, allowing us to navigate in the tiktok video (left click on timeline teleports on the given timing, right click teleports to start frame of the scene we clicked on). On the timeline we need to see the cursor (where we're currently) and the scenes (its number and a different background color to rapidly visually differentiate them).
- Controls (on or under player) to navigate in the video. Should include play / pause and previous frame + next frame.
- Depending on space available (either on the right or under, pick best UX-wise), display the **current scene** information (this depends on where timeline cursor is, this section will automatically be updated and display the correct current scene if we move through timeline or play the tiktok video). It will feature 5 buttons: on extremities will be two buttons to set start and end timing of the scene on **current cursor's timing**. Think about edge cases (for example if we set start timing of clip number 1 at something different than 00:00 it will automatically create another scene before. This means this newly created clip becomes new number 1 and our clip becomes number 2 (this means all other clips will shift)). The two more inner buttons will then be a merge with previous scene and a merge with next scene button. They will remove respectively start timing or end timing and engulf the adjacent corresponding scene. And the last button (at center) will be a "split" button. It will split the scene in two.
  Scene information will also display concise information like scene number, start timing and end timing (as a text field, they can be changed manually and will behave under the hood like set start timing button / set end timing button but with a custom value and not cursor's current timestamp. Handle edge cases and refuse unauthorized timestamp) and scene duration.
- RULES: the timeline **cannot have** gaps with no clip. Clips are named by their order (1, 2, 3...) so if shifting occurs change scenes' name accordingly.
